{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Object detection algorithms usually sample a large number of regions in the input image, determine whether these regions contain objects of interest, and adjust the boundaries of the regions so as to predict the ground-truth bounding boxes of the objects more accurately. Different models may adopt different region sampling schemes. Here we introduce one of such methods: it generates multiple bounding boxes with varying scales and aspect ratios centered on each pixel.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Mapping' from 'collections' (A:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\collections\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatplotlib\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minline\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01md2l\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m torch \u001B[38;5;28;01mas\u001B[39;00m d2l\n\u001B[0;32m      5\u001B[0m torch\u001B[38;5;241m.\u001B[39mset_printoptions(\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\d2l\\torch.py:6\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\torchvision\\__init__.py:5\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m io\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_optical_flow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KittiFlow, Sintel, FlyingChairs, FlyingThings3D, HD1K\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcaltech\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Caltech101, Caltech256\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mceleba\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CelebA\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\torchvision\\datasets\\_optical_flow.py:13\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _read_png_16\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m verify_str_arg\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VisionDataset\n\u001B[0;32m     17\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKittiFlow\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSintel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHD1K\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     23\u001B[0m )\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:21\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Callable, List, Iterable, Optional, TypeVar, Dict, IO, Tuple, Iterator\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m urlparse\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_zoo\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\requests\\__init__.py:43\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# -*- coding: utf-8 -*-\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#   __\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m#  /__)  _  _     _   _ _/   _\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# / (   (- (/ (/ (- _)  /  _)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#          /\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03mRequests HTTP Library\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m~~~~~~~~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;124;03m:license: Apache 2.0, see LICENSE for more details.\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib3\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\urllib3\\__init__.py:8\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m absolute_import\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnectionpool\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      9\u001B[0m     HTTPConnectionPool,\n\u001B[0;32m     10\u001B[0m     HTTPSConnectionPool,\n\u001B[0;32m     11\u001B[0m     connection_from_url\n\u001B[0;32m     12\u001B[0m )\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m exceptions\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfilepost\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m encode_multipart_formdata\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:29\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackages\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m six\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackages\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msix\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmoves\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m queue\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     30\u001B[0m     port_by_scheme,\n\u001B[0;32m     31\u001B[0m     DummyConnection,\n\u001B[0;32m     32\u001B[0m     HTTPConnection, HTTPSConnection, VerifiedHTTPSConnection,\n\u001B[0;32m     33\u001B[0m     HTTPException, BaseSSLError,\n\u001B[0;32m     34\u001B[0m )\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RequestMethods\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\urllib3\\connection.py:39\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     32\u001B[0m     NewConnectionError,\n\u001B[0;32m     33\u001B[0m     ConnectTimeoutError,\n\u001B[0;32m     34\u001B[0m     SubjectAltNameWarning,\n\u001B[0;32m     35\u001B[0m     SystemTimeWarning,\n\u001B[0;32m     36\u001B[0m )\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackages\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mssl_match_hostname\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m match_hostname, CertificateError\n\u001B[1;32m---> 39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mssl_\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     40\u001B[0m     resolve_cert_reqs,\n\u001B[0;32m     41\u001B[0m     resolve_ssl_version,\n\u001B[0;32m     42\u001B[0m     assert_fingerprint,\n\u001B[0;32m     43\u001B[0m     create_urllib3_context,\n\u001B[0;32m     44\u001B[0m     ssl_wrap_socket\n\u001B[0;32m     45\u001B[0m )\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m connection\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_collections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPHeaderDict\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\urllib3\\util\\__init__.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m absolute_import\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# For backwards compatibility, provide imports that used to be here.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_connection_dropped\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_headers\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_fp_closed\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m absolute_import\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msocket\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwait\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wait_for_read\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mselectors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HAS_SELECT, SelectorError\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_connection_dropped\u001B[39m(conn):  \u001B[38;5;66;03m# Platform-specific\u001B[39;00m\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\urllib3\\util\\wait.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mselectors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     HAS_SELECT,\n\u001B[0;32m      3\u001B[0m     DefaultSelector,\n\u001B[0;32m      4\u001B[0m     EVENT_READ,\n\u001B[0;32m      5\u001B[0m     EVENT_WRITE\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wait_for_io_events\u001B[39m(socks, events, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124;03m\"\"\" Waits for IO events to be available from a list of sockets\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m    or optionally a single socket if passed in. Returns a list of\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m    sockets that can be interacted with immediately. \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32mA:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\urllib3\\util\\selectors.py:14\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m namedtuple, Mapping\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     monotonic \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'Mapping' from 'collections' (A:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\collections\\__init__.py)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "torch.set_printoptions(2)  # Simplify printing accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Multiple Anchor Boxes\n",
    "Suppose that the input image has a height of h and width of w. We generate anchor boxes with different shapes centered on each pixel of the image. Let the scale be s $ \\in $[0, 1] and the aspect ratio (ratio of width to height) is r > 0. Then the width and height of the anchor box are w * s * $ \\sqrt{r} $ and hs/$ \\sqrt{r} $, respectively. Note that when the center position is given, an anchor box with known width and height is determined.\n",
    "\n",
    "To generate multiple anchor boxes with different shapes, let’s set a series of scales s1, s2, ...sn  and a series of aspect ratios r1, r2, ... rn. When using all the combinations of these scales and aspect ratios with each pixel as the center, the input image will have a total of w * h * s *r  anchor boxes.\n",
    " Although these anchor boxes may cover all the ground-truth bounding boxes, the computational complexity is easily too high. In practice, we can only consider those combinations containing s1 or r1 :\n",
    " (s1, r1),(s2, r2),....(s1, rn),(s2, r1),(s3, r1),....(sm, r1)\n",
    " That is to say, the number of anchor boxes centered on the same pixel is (n + m -1). For the entire input image, we will generate a total of w * h * (n + m - 1) anchor boxes.\n",
    " The above method of generating anchor boxes is implemented in the following multibox_prior function. We specify the input image, a list of scales, and a list of aspect ratios, then this function will return all the anchor boxes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def multibox_prior(data, size, ratios):\n",
    "    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.device, len(size), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    size_tensor = torch.tensor(size, device=device)\n",
    "    ratio_tensor = torch.tensor(ratios, device=device)\n",
    "    # Offsets are required to move the anchor to the center of a pixel. Since\n",
    "    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n",
    "    offset_h, offset_w = 0.5, 0.5\n",
    "    steps_h = 1.0 / in_height # Scaled steps in y axis\n",
    "    steps_w = 1.0 / in_width # Scaled steps in x axis\n",
    "\n",
    "    # Generate all center points for the anchor boxes\n",
    "    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n",
    "    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n",
    "    shift_y, shift_x = torch.meshgrid(center_h, center_w)\n",
    "    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n",
    "\n",
    "    # Generate boxes_per_pixel number of heights and witdh that are later used\n",
    "    # to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n",
    "    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n",
    "                   size[0] * torch.sqrt(ratio_tensor[1:]))) * in_height / in_width   # handle rectangular input\n",
    "    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n",
    "                   size[0] / torch.sqrt(ratio_tensor[1:])))\n",
    "    # Divide by 2 to get half height and half width\n",
    "    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n",
    "                                        in_height * in_width, 1) / 2\n",
    "    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n",
    "    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n",
    "    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n",
    "                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n",
    "    output = out_grid + anchor_manipulations\n",
    "    return output.unsqueeze(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the shape of the returned anchor box variable Y is (batch size, number of anchor boxes, 4)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = d2l.plt.imread('catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "print(h, w)\n",
    "X = torch.rand(size=(1, 3, h, w))  # Construct input data\n",
    "Y = multibox_prior(X, size=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img.shape[:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After changing the shape of the anchor box variable Y to (image height, image width, number of anchor boxes centered on the same pixel, 4), we can obtain all the anchor boxes centered on a specified pixel position. In the following, we access the first anchor box centered on (250, 250). It has four elements: the (x,y)-axis coordinates at the upper-left corner and the (x,y)-axis coordinates at the lower-right corner of the anchor box. The coordinate values of both axes are divided by the width and height of the image, respectively; thus, the range is between 0 and 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "boxes = Y.reshape(h, w, 5, 4)\n",
    "boxes[250, 250, 0, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to show all the anchor boxes centered on one pixel in the image, we define the following show_bboxes function to draw multiple bounding boxes on the image."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_bboxes(axes, bboxes, labels = None, colors = None):\n",
    "    def make_list(obj, default_values = None):\n",
    "        if obj is None:\n",
    "            obj = default_values\n",
    "        elif not isinstance(obj, (list, tuple)):\n",
    "            obj = [obj]\n",
    "        return obj\n",
    "\n",
    "    labels = make_list(labels)\n",
    "    colors = make_list(['b', 'g', 'r', 'm', 'c'])\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        color = colors[i % len(colors)]\n",
    "        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)\n",
    "        axes.add_patch(rect)\n",
    "        if labels and len(labels) > i:\n",
    "            text_color = 'k' if color == 'w' else 'w'\n",
    "            axes.text(rect.xy[0], rect.xy[1], labels[i],\n",
    "                      va='center', ha='center', fontsize=9, color=text_color,\n",
    "                      bbox=dict(facecolor=color, lw=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we just saw, the coordinate values of the x and y axes in the variable boxes have been divided by the width and height of the image, respectively. When drawing anchor boxes, we need to restore their original coordinate values; thus, we define variable bbox_scale below. Now, we can draw all the anchor boxes centered on (250, 250) in the image. As you can see, the blue anchor box with a scale of 0.75 and an aspect ratio of 1 well surrounds the dog in the image."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "bbox_scale = torch.tensor((w, h, w, h))\n",
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,\n",
    "            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',\n",
    "             's=0.75, r=0.5'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intersection over Union (IoU)\n",
    "$ J(A, B) = \\frac{A \\cap B}{A \\cup B}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"Compute pairwise IoU across two list of anchor box or bouding box\"\"\"\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))\n",
    "    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n",
    "    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\n",
    "    areas1 = box_area(boxes1)\n",
    "    areas2 = box_area(boxes2)\n",
    "    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n",
    "    # boxes1, no. of boxes2, 2)\n",
    "    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n",
    "    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Labeling Anchor Boxes in Training Data\n",
    "In a training dataset, we consider each anchor box as a training example\n",
    "In order to train an object detection model, we need class and offset labels for each anchor box.\n",
    "1. class: is the class of the object relevant to the anchor box\n",
    "2. offset: the offset of the ground-truth bounding box relative to the anchor box\n",
    " During the prediction, for each image we generate multiple anchor boxes, predict classes and offsets for all the anchor boxes, adjust their positions according to the predicted offsets to obtain the predicted bounding boxes, and finally only output those predicted bounding boxes that satisfy certain criteria.\n",
    "\n",
    "An object detection training set comes with labels for locations of ground-truth bounding boxes and classes of their surrounded objects.\n",
    "To label any generated anchor box, we refer to the labeled location and class of its assigned ground-truth bounding box that is closest to the anchor box.\n",
    "In the following, we describe an algorithm for assigning closest ground-truth bounding boxes to anchor boxes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Assigning Ground-Truth Bounding Boxes to Anchor Boxes\n",
    "Given an image, suppose that the anchor boxes are A1, .. Ana and the ground-truth bounding boxes are B1, ... Bnb, where $ \\ n_a \\geq \\ n_b $\n",
    "et’s define a matrix X $ \\in \\ R^{\\ n_a*\\ n_b} $ whose element $\\ x_ij$ in the $ \\ i^th$ row and $\\ j^th$ column is the IoU of the anchor box $\\ A_i $ and the ground-truth bounding box $\\ B_j $ The algorithm consists of the following steps:\n",
    "1. Find the largest element in matrix X and denote its row and column indices as $ \\ i_1$  and $\\ j_1$, respectively. Then the ground-truth bounding box  $\\ B_j_1 $ is assigned to the anchor box $\\ A_i_1 $. This is quite intuitive because $\\ A_i_1 $ and $\\ B_j_1 $ are the closest among all the pairs of anchor boxes and ground-truth bounding boxes. After the first assignment, discard all the elements in the  row $ \\ i_\n",
    "   {1}^{th} $ and the $ \\ j_\n",
    "   {1}^{th} $ column in matrix X.\n",
    "\n",
    "2. Find the largest of the remaining elements in matrix X and denote its row and column indices as $ \\ i_2$  and $\\ j_2$ , respectively. We assign ground-truth bounding box  $\\ B_j_2 $to anchor box  $\\ A_i_1 $ and discard all the elements in the  row $ \\ i_2$  and the  $\\ j_2$ column in matrix X.\n",
    "3. At this point, elements in two rows and two columns in matrix X have been discarded. We proceed until all elements in $\\ n_b $ columns in matrix X are discarded. At this time, we have assigned a ground-truth bounding box to each of $\\ n_b $ anchor boxes.\n",
    "4.  Only traverse through the remaining $\\ n_b - \\ n_b $  anchor boxes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n",
    "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n",
    "    # box i and the ground-truth bounding box j\n",
    "    jaccard = box_iou(anchors, ground_truth)\n",
    "    # each anchor\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n",
    "                                  device=device)\n",
    "     # Assign ground-truth bounding boxes according to the threshold\n",
    "    max_ious, indices = torch.max(jaccard, dim=1)\n",
    "    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\n",
    "    box_j = indices[max_ious >= iou_threshold]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "    col_discard = torch.full((num_anchors,), -1)\n",
    "    row_discard = torch.full((num_gt_boxes,), -1)\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n",
    "        box_idx = (max_idx % num_gt_boxes).long()\n",
    "        anc_idx = (max_idx / num_gt_boxes).long()\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "    return anchors_bbox_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Labeling Classes and Offsets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can label the class and offset for each anchor box. Suppose that an anchor box A is assigned a ground-truth bounding box B. On the one hand, the class of the anchor box A will be labeled as that of B. On the other hand, the offset of the anchor box  will be labeled according to the relative position between the central coordinates of A  and B together with the relative size between these two boxes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def offset_boxes(anchors, assigned_bb, eps=1e-6):\n",
    "    \"\"\"Transform for anchor box offsets.\"\"\"\n",
    "    c_anc = d2l.box_corner_to_center(anchors)\n",
    "    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)\n",
    "    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\n",
    "    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\n",
    "    offset = torch.cat([offset_xy, offset_wh], axis=1)\n",
    "    return offset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If an anchor box is not assigned a ground-truth bounding box, we just label the class of the anchor box as “background”. Anchor boxes whose classes are background are often referred to as negative anchor boxes, and the rest are called positive anchor boxes. We implement the following multibox_target function to label classes and offsets for anchor boxes (the anchors argument) using ground-truth bounding boxes (the labels argument). This function sets the background class to zero and increments the integer index of a new class by one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def multibox_target(anchors, labels):\n",
    "    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    device, num_anchors = anchors.device, anchors.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            label[:, 1:], anchors, device)\n",
    "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n",
    "            1, 4)\n",
    "        # Initialize class labels and assigned bounding box coordinates with\n",
    "        # zeros\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n",
    "                                   device=device)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n",
    "                                  device=device)\n",
    "        # Label classes of anchor boxes using their assigned ground-truth\n",
    "        # bounding boxes. If an anchor box is not assigned any, we label its\n",
    "        # class as background (the value remains zero)\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "        # Offset transformation\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An Example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                         [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                    [0.57, 0.3, 0.92, 0.9]])\n",
    "\n",
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')\n",
    "show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = multibox_target(anchors.unsqueeze(dim=0),\n",
    "                         ground_truth.unsqueeze(dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting Bounding Boxes with Non-Maximum Suppression\n",
    "During prediction, we generate multiple anchor boxes for the image and predict classes and offsets for each of them. A predicted bounding box is thus obtained according to an anchor box with its predicted offset. Below we implement the offset_inverse function that takes in anchors and offset predictions as inputs and applies inverse offset transformations to return the predicted bounding box coordinates.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@save\n",
    "def offset_inverse(anchors, offset_preds):\n",
    "    \"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\n",
    "    anc = d2l.box_corner_to_center(anchors)\n",
    "    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]\n",
    "    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]\n",
    "    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)\n",
    "    predicted_bbox = d2l.box_center_to_corner(pred_bbox)\n",
    "    return predicted_bbox"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When there are many anchor boxes, many similar (with significant overlap) predicted bounding boxes can be potentially output for surrounding the same object. To simplify the output, we can merge similar predicted bounding boxes that belong to the same object by using non-maximum suppression (NMS)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is how non-maximum suppression works.  For a predicted bounding box B . Denoting by p the largest predicted likelihood, the class corresponding to this probability is the predicted class for B.\n",
    "Specifically, we refer to p as the confidence (score) of the predicted bounding box B\n",
    "On the same image, all the predicted non-background bounding boxes are sorted by confidence in descending order to generate a list L.\n",
    "Then we manipulate the sorted list L in the following steps:\n",
    "##### Input\n",
    "We get a list *P* of prediction BBoxes of the form (x1,y1,x2,y2,c), where (x1,y1) and (x2,y2) are the ends of the BBox and c is the predicted confidence score of the model. We also get overlap threshold IoU thresh_iou.\n",
    "##### Output\n",
    "We return a list keep of filtered prediction BBoxes\n",
    "##### Algorithm\n",
    "1. Select the prediction S with highest confidence score and remove it from P and add it to the final prediction list *keep*. (keep is empty initially).\n",
    "2. Now compare this prediction S with all the predictions present in P. Calculate the IoU of this prediction S with every other predictions in P. If the IoU is greater than the threshold thresh_iou for any prediction T present in P, remove prediction T from P.\n",
    "3. If there are still predictions left in P, then go to Step 1 again, else return the list keep containing the filtered predictions.\n",
    "\n",
    "In layman terms, we select the predictions with the maximum confidence and suppress all the other predictions having overlap with the selected predictions greater than a threshold. In other words, we take the maximum and suppress the non-maximum ones, hence the name non-maximum suppression.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_threshold):\n",
    "    \"\"\"Sort confidence score of predicted bouding boxes\"\"\"\n",
    "    B = torch.argsort(scores, dim=-1, descending=True)\n",
    "    keep = []    # Indices of predicted bounding boxes that will be kept\n",
    "    while B.numel() > 0:\n",
    "        i = B[0]\n",
    "        keep.append(i)\n",
    "        if B.numel() == 1:\n",
    "            break\n",
    "        iou = box_iou(boxes[i, :].reshape(-1, 4),\n",
    "              boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\n",
    "        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)\n",
    "        B = B[inds + 1]\n",
    "    return torch.tensor(keep, device=boxes.device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define the following multibox_detection to apply non-maximum suppression to predicting bounding boxes. Do not worry if you find the implementation a bit complicated: we will show how it works with a concrete example right after the implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n",
    "                       pos_threshold=0.009999999):\n",
    "    \"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\n",
    "    device, batch_size = cls_probs.device, cls_probs.shape[0]\n",
    "    anchors = anchors.squeeze(0)\n",
    "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n",
    "    out = []\n",
    "    for i in range(batch_size):\n",
    "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n",
    "        conf, class_id = torch.max(cls_prob[1:], 0)\n",
    "        predicted_bb = offset_inverse(anchors, offset_pred)\n",
    "        keep = nms(predicted_bb, conf, nms_threshold)\n",
    "        # Find all non-`keep` indices and set the class to background\n",
    "        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)\n",
    "        combined = torch.cat((keep, all_idx))\n",
    "        uniques, counts = combined.unique(return_counts=True)\n",
    "        non_keep = uniques[counts == 1]\n",
    "        all_id_sorted = torch.cat((keep, non_keep))\n",
    "        class_id[non_keep] = -1\n",
    "        class_id = class_id[all_id_sorted]\n",
    "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n",
    "        # Here `pos_threshold` is a threshold for positive (non-background) predictions\n",
    "        below_min_idx = (conf < pos_threshold)\n",
    "        class_id[below_min_idx] = -1\n",
    "        conf[below_min_idx] = 1 - conf[below_min_idx]\n",
    "        pred_info = torch.cat((class_id.unsqueeze(1),\n",
    "                               conf.unsqueeze(1),\n",
    "                               predicted_bb), dim=1)\n",
    "        out.append(pred_info)\n",
    "    return torch.stack(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n",
    "                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\n",
    "offset_preds = torch.tensor([0] * anchors.numel())\n",
    "cls_probs = torch.tensor([[0] * 4,  # Predicted background likelihood\n",
    "                      [0.9, 0.8, 0.7, 0.1],  # Predicted dog likelihood\n",
    "                      [0.1, 0.2, 0.3, 0.9]])  # Predicted cat likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n",
    "                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\n",
    "offset_preds = torch.tensor([0] * anchors.numel())\n",
    "cls_probs = torch.tensor([[0] * 4,  # Predicted background likelihood\n",
    "                      [0.9, 0.8, 0.7, 0.1],  # Predicted dog likelihood\n",
    "                      [0.1, 0.2, 0.3, 0.9]])  # Predicted cat likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, anchors * bbox_scale,\n",
    "            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = multibox_detection(cls_probs.unsqueeze(dim=0),\n",
    "                            offset_preds.unsqueeze(dim=0),\n",
    "                            anchors.unsqueeze(dim=0),\n",
    "                            nms_threshold=0.5)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = d2l.plt.imshow(img)\n",
    "for i in output[0].detach().numpy():\n",
    "    if i[0] == -1:\n",
    "        continue\n",
    "    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])\n",
    "    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. We generate anchor boxes with different shapes centered on each pixel of the image.\n",
    "2. Intersection over union (IoU), also known as Jaccard index, measures the similarity of two bounding boxes. It is the ratio of their intersection area to their union area.\n",
    "3. In a training set, we need two types of labels for each anchor box. One is the class of the object relevant to the anchor box and the other is the offset of the ground-truth bounding box relative to the anchor box.\n",
    "4. During prediction, we can use non-maximum suppression (NMS) to remove similar predicted bounding boxes, thereby simplifying the output."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}